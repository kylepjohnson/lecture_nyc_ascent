{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import csv\n",
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Parse dates correctly; this is close but not working\n",
    "date_parser = lambda x: pandas.datetime.strptime(x, '%a %b %d %H:%M:%S +z %Y')  # Mon Feb 15 20:44:33 +0000 2016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_df(csv_file):\n",
    "    \"\"\"Open csv, return Pandas DataFrame.\"\"\"\n",
    "    dataframe = pandas.read_csv(csv_file, \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False, \n",
    "                             parse_dates=True,\n",
    "                             date_parser=date_parser\n",
    "                            )\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# rows, columns\n",
    "#print('Shape before:', dataframe_popular.shape)\n",
    "\n",
    "#popular_df = dataframe_popular.drop_duplicates()\n",
    "#print('Shape after:', dataframe_popular.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other cleanup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Todo\n",
    "\n",
    "# case, space, some punctuation, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "\n",
    "Show plain function, maybe NLTK too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer\n",
    "\n",
    "def tokenize_words(input_string):\n",
    "    \"\"\"Take a string, return a list of \n",
    "    strings broken on whitespace, but do \n",
    "    not break @mentions and URLs.\n",
    "    \"\"\"\n",
    "    punctuation = [',', '!', '\"', '. ', ': ']\n",
    "    for char in punctuation:\n",
    "        input_string = input_string.replace(char, ' ')\n",
    "    \n",
    "    return [w for w in input_string.split(' ') if w]  # rm empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@CuteEmergency', \"I'm\", 'okay', 'https://t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See @users and http: not split\n",
    "a_tweet = \"\"\"@CuteEmergency: \"I'm okay!\" https://t.co/TWMwjG03Fd\"\"\"\n",
    "tokenize_words(a_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting text\n",
    "\n",
    "* count chars\n",
    "* count words\n",
    "* links\n",
    "* count links\n",
    "* #hashtags\n",
    "* count #hashtags\n",
    "* @mentions\n",
    "* count @mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `http(s)://`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('http')]`\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('http'):\n",
    "            urls.append(word)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `#`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('#')]`\n",
    "    \"\"\"\n",
    "    hashtags = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('#'):\n",
    "            hashtags.append(word)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `@`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('@')]`\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('@'):\n",
    "            mentions.append(word)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features_to_df(dataframe):\n",
    "    \"\"\"Take DataFrame of tweets, extract some specific \n",
    "    features and add to returned DataFrame.\n",
    "    \"\"\"\n",
    "    #tokens = []  # list of strings\n",
    "    char_count = []\n",
    "    word_count = []\n",
    "    urls = []\n",
    "    hashtags = []\n",
    "    mentions = []\n",
    "\n",
    "    for i, row in dataframe.iterrows():\n",
    "        \n",
    "        # Text and tokens\n",
    "        tokens = tokenize_words(row['_text'])\n",
    "        char_count.append(len(row['_text']))\n",
    "        word_count.append(len(tokens))\n",
    "\n",
    "        # URLs\n",
    "        url_list = get_urls(tokens)\n",
    "        urls.append(url_list)\n",
    "        url_count = len(url_list)\n",
    "\n",
    "        # Hashtags\n",
    "        hashtag_list = get_hashtags(tokens)\n",
    "        hashtags.append(hashtag_list)\n",
    "        hashtag_count = len(hashtag_list)\n",
    "\n",
    "        # Mentions\n",
    "        mentions_list = get_mentions(tokens)\n",
    "        mentions.append(mentions_list)\n",
    "        mentions_count = len(mentions_list)\n",
    "\n",
    "    dataframe['_char_count'] = char_count\n",
    "    dataframe['_word_count'] = word_count\n",
    "    dataframe['_urls'] = urls\n",
    "    dataframe['_url_count'] = url_count\n",
    "    dataframe['_hashtags'] = hashtags\n",
    "    dataframe['_hashtag_count'] = hashtag_count\n",
    "    dataframe['_mentions'] = hashtags\n",
    "    dataframe['_mentions_count'] = mentions_count\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Named entity recognition (NER)\n",
    "\n",
    "Maybe show NLTK code, but don't do, too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "\n",
    "Helpful links:\n",
    "* <http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation>\n",
    "* <https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_merge_bow_write(dataframe, save_path):\n",
    "    \"\"\"Take a dataframe, extract '_text' and make a Bag of Words.\n",
    "    Write BoW features to their own file, then merge with input\n",
    "    and return new dataframe.\n",
    "    \n",
    "    TODO: Revisit options for CountVectorizer() (lowercase, tokenizer, min freq)\n",
    "    \"\"\"\n",
    "    # Get list of strings, for input into vectorizer\n",
    "    text_list = dataframe['_text'].tolist()\n",
    "\n",
    "    # Setup Vectorizer\n",
    "    # Note that min_df is confusing; see http://stackoverflow.com/a/27697863\n",
    "    # min_df + an integer: if word found in less than n docs, then ignore\n",
    "    vectorizer = CountVectorizer(min_df=2)  \n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)  # input is a list of strings, 1 per document\n",
    "\n",
    "    # Put BoW vectors into a new df\n",
    "    dataframe_bow = pandas.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # Write BoW to disk\n",
    "    # Just the Bag of Words, in case we want to use it by itself later\n",
    "    dataframe_bow.to_csv(save_path, sep='|', encoding='utf-8')\n",
    "    \n",
    "    # Merge BoW df with the original feature table df\n",
    "    # Important: Make sure the concat() function uses the original id index of the first, text datafram\n",
    "    dataframe = pandas.concat([dataframe, dataframe_bow], axis=1, join_axes=[dataframe.index])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_merge_bow(dataframe):\n",
    "    \"\"\"Take a dataframe, extract '_text' and make a Bag of Words.\n",
    "    Write BoW features to their own file, then merge with input\n",
    "    and return new dataframe.\n",
    "    \n",
    "    TODO: Revisit options for CountVectorizer() (lowercase, tokenizer, min freq)\n",
    "    \"\"\"\n",
    "    # Get list of strings, for input into vectorizer\n",
    "    text_list = dataframe['_text'].tolist()\n",
    "\n",
    "    # Setup Vectorizer\n",
    "    # Note that min_df is confusing; see http://stackoverflow.com/a/27697863\n",
    "    # min_df + an integer: if word found in less than n docs, then ignore\n",
    "    vectorizer = CountVectorizer(min_df=2)  \n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)  # input is a list of strings, 1 per document\n",
    "\n",
    "    # Put BoW vectors into a new df\n",
    "    dataframe_bow = pandas.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return dataframe_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Think about how to put into feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write entire DataFrame to csv\n",
    "\n",
    "The next notebook will pick up from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Do everything again for the unpopular tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_all_features_for_tweets():\n",
    "    \"\"\"Do all the steps to create one feature \n",
    "    table of popular and unpopular tweets.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make sure 'feature_tables' present\n",
    "    features_dir = 'feature_tables'\n",
    "    if not os.path.isdir(features_dir):\n",
    "        os.mkdir(features_dir)\n",
    "\n",
    "    # load csvs to dfs\n",
    "    dataframe_popular = csv_to_df('tweets/tweets_popular.csv')\n",
    "    dataframe_not_popular = csv_to_df('tweets/tweets_not_popular.csv')\n",
    "    \n",
    "    # rm dupes\n",
    "    dataframe_popular = dataframe_popular.drop_duplicates()\n",
    "    dataframe_not_popular = dataframe_not_popular.drop_duplicates()\n",
    "    \n",
    "    # add column '_popular' or '_unpopular' for each df\n",
    "    dataframe_popular['_popular'] = True\n",
    "    dataframe_not_popular['_popular'] = False\n",
    "    \n",
    "    # append unpopular to popular df\n",
    "    dataframe = pandas.concat([dataframe_popular, dataframe_not_popular])\n",
    "\n",
    "    dataframe = add_features_to_df(dataframe)\n",
    "    dataframe.to_csv('feature_tables/basics.csv', sep='|', encoding='utf-8')\n",
    "    #dataframe = make_merge_bow_write(dataframe, 'feature_tables/bow.csv')  # this both writes and returns a df -- bad practice due to memory issues\n",
    "\n",
    "    # Make BoW df, then write it to .csv\n",
    "    \n",
    "    dataframe_bow = make_merge_bow(dataframe)\n",
    "    \n",
    "    # Just the Bag of Words, in case we want to use it by itself later\n",
    "    dataframe_bow.to_csv('feature_tables/bow.csv', sep='|', encoding='utf-8')\n",
    "    \n",
    "    # Merge BoW df with the original feature table df\n",
    "    # Important: Make sure the concat() function uses the original id index of the first, text df\n",
    "    dataframe = pandas.concat([dataframe, dataframe_bow], axis=1, join_axes=[dataframe.index])\n",
    "\n",
    "\n",
    "    print('Total (rows, columns):', dataframe.shape)  # (rows, columns)\n",
    "\n",
    "    dataframe.to_csv('feature_tables/all.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (rows, columns): (22706, 11014)\n"
     ]
    }
   ],
   "source": [
    "make_all_features_for_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
