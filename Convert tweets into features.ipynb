{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open with csv module, iterate row-by-row\n",
    "with open('tweets/tweets_popular.csv', 'rb') as file_open:\n",
    "    popular_csv = csv.reader(file_open, delimiter='|')\n",
    "    for row in popular_csv:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open with Pandas, load into DataFrame\n",
    "\n",
    "# TODO: Parse dates correctly; this is close but not working\n",
    "date_parser = lambda x: pandas.datetime.strptime(x, '%a %b %d %H:%M:%S +z %Y')  # Mon Feb 15 20:44:33 +0000 2016\n",
    "\n",
    "popular_df = pandas.read_csv('tweets/tweets_popular.csv', \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False, \n",
    "                             parse_dates=True,\n",
    "                             date_parser=date_parser\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (4571, 3))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our data\n",
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column\n",
      "0    @CringeLMAO: Easy there m8 https://t.co/dnF3Wq...\n",
      "1    @AustinMahone: Just posted a photo https://t.c...\n",
      "2    @Ashton5SOS: Some days I drink way to much cof...\n",
      "3    @lailamuhammad: When you nail that #Beyonc   m...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Text column')\n",
    "print(popular_df['text'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet count\n",
      "0     2084\n",
      "1     1059\n",
      "2    24121\n",
      "3      801\n",
      "Name: rt_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Retweet count')\n",
    "print(popular_df['rt_count'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date-time\n",
      "0    Mon Feb 15 20:44:33 +0000 2016\n",
      "1    Mon Feb 15 20:44:33 +0000 2016\n",
      "2    Mon Feb 15 20:44:33 +0000 2016\n",
      "3    Mon Feb 15 20:44:33 +0000 2016\n",
      "Name: tweet_datetime, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Date-time')\n",
    "print(popular_df['tweet_datetime'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the parsed date-time\n",
    "# TODO: Try to parse this right\n",
    "dt = popular_df['tweet_datetime'][0]\n",
    "print(type(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (18093, 3))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "# Do the same for unpopular data\n",
    "not_popular_df = pandas.read_csv('tweets/tweets_not_popular.csv', \n",
    "                                 delimiter='|', \n",
    "                                 error_bad_lines=False, \n",
    "                                 warn_bad_lines=False,\n",
    "                                 parse_dates=True,\n",
    "                                 date_parser=date_parser)\n",
    "\n",
    "# Let's inspect our data\n",
    "print('Shape:', not_popular_df.shape)\n",
    "print('Columns:', not_popular_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape before', (4571, 3))\n",
      "('Shape after', (4400, 3))\n"
     ]
    }
   ],
   "source": [
    "print('Shape before', popular_df.shape)\n",
    "popular_df = popular_df.drop_duplicates()\n",
    "print('Shape after', popular_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape before', (18093, 3))\n",
      "('Shape after', (18093, 3))\n"
     ]
    }
   ],
   "source": [
    "print('Shape before', not_popular_df.shape)\n",
    "popular_df = not_popular_df.drop_duplicates()\n",
    "print('Shape after', not_popular_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other cleanup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "\n",
    "Show plain function, maybe NLTK too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer\n",
    "\n",
    "def tokenize_words(input_string):\n",
    "    \"\"\"Take a string, return a list of \n",
    "    strings broken on whitespace, but do \n",
    "    not break @mentions and URLs.\n",
    "    \"\"\"\n",
    "    punctuation = [',', '!', '\"', '. ', ': ']\n",
    "    for char in punctuation:\n",
    "        input_string = input_string.replace(char, ' ')\n",
    "    \n",
    "    return [w for w in input_string.split(' ') if w]  # rm empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@taylorcaniff',\n",
       " 'Never',\n",
       " 'mind',\n",
       " \"I'm\",\n",
       " 'snowed',\n",
       " 'in',\n",
       " 'again',\n",
       " 'I',\n",
       " \"can't\",\n",
       " 'quit',\n",
       " 'laughing']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tweet = \"@taylorcaniff: Never mind I'm snowed in again I can't quit laughing\"\n",
    "tokenize_words(a_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@CuteEmergency', \"I'm\", 'okay', 'https://t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See @users and http: not split\n",
    "another_tweet = \"\"\"@CuteEmergency: \"I'm okay!\" https://t.co/TWMwjG03Fd\"\"\"\n",
    "tokenize_words(another_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the Python re library \n",
    "def tokenize_words_regex(input_string):\n",
    "    \"\"\"Tokenize input string with re library,\n",
    "    return list of strings.\"\"\"\n",
    "    tokenization_regex = re.compile(r\"[\\w']+|[.,!?;]\")\n",
    "    return tokenization_regex.findall(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CuteEmergency', \"I'm\", 'okay', '!', 'https', 't', '.', 'co', 'TWMwjG03Fd']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_words_regex(another_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'CuteEmergency',\n",
       " ':',\n",
       " '\"',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'okay',\n",
       " '!',\n",
       " '\"',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK has one, too but still breaks up what we need,\n",
    "# we'll skip for this exercise\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "\n",
    "nltk_tokenizer = PunktLanguageVars()\n",
    "nltk_tokenizer.word_tokenize(another_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Todo\n",
    "\n",
    "case, space, some punctuation, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add column to datafram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "# TODO do with .loc, not copy\n",
    "\n",
    "tokens = []  # list of strings\n",
    "\n",
    "for i, row in popular_df.iterrows():\n",
    "    tokens.append(tokenize_words(row['text'])) \n",
    "\n",
    "popular_df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (17682, 4))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime', u'tokens'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'text', u'rt_count', u'tweet_datetime', u'tokens'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Repeat for not_popular\n",
    "\n",
    "tokens = []  # list of strings\n",
    "\n",
    "for i, row in not_popular_df.iterrows():\n",
    "    tokens.append(tokenize_words(row['text'])) \n",
    "\n",
    "not_popular_df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition (NER)\n",
    "\n",
    "Show NLTK code, not for feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting text\n",
    "\n",
    "* count chars\n",
    "* count words\n",
    "* links\n",
    "* count links\n",
    "* #hashtags\n",
    "* count #hashtags\n",
    "* @mentions\n",
    "* count @mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `http(s)://`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('http')]`\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('http'):\n",
    "            urls.append(word)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `#`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('#')]`\n",
    "    \"\"\"\n",
    "    hashtags = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('#'):\n",
    "            hashtags.append(word)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `@`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('@')]`\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('@'):\n",
    "            mentions.append(word)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "# TODO do with .loc, not copy\n",
    "\n",
    "char_count = []\n",
    "word_count = []\n",
    "urls = []\n",
    "hashtags = []\n",
    "mentions = []\n",
    "\n",
    "for i, row in popular_df.iterrows():\n",
    "    # Text and tokens\n",
    "    char_count.append(len(row['text']))\n",
    "    word_count.append(len(row['tokens']))\n",
    "    \n",
    "    # URLs\n",
    "    url_list = get_urls(row['tokens'])\n",
    "    urls.append(url_list)\n",
    "    url_count = len(url_list)\n",
    "    \n",
    "    # Hashtags\n",
    "    hashtag_list = get_hashtags(row['tokens'])\n",
    "    hashtags.append(hashtag_list)\n",
    "    hashtag_count = len(hashtag_list)\n",
    "    \n",
    "    # Mentions\n",
    "    mentions_list = get_mentions(row['tokens'])\n",
    "    mentions.append(mentions_list)\n",
    "    mentions_count = len(mentions_list)\n",
    "\n",
    "\n",
    "popular_df['char_count'] = char_count\n",
    "popular_df['word_count'] = word_count\n",
    "popular_df['urls'] = urls\n",
    "popular_df['url_count'] = url_count\n",
    "popular_df['hashtags'] = hashtags\n",
    "popular_df['hashtag_count'] = hashtag_count\n",
    "popular_df['mentions'] = hashtags\n",
    "popular_df['mentions_count'] = mentions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (17682, 12))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime', u'tokens', u'char_count',\n",
      "       u'word_count', u'urls', u'url_count', u'hashtags', u'hashtag_count',\n",
      "       u'mentions', u'mentions_count'],\n",
      "      dtype='object'))\n",
      "Index([u'text', u'rt_count', u'tweet_datetime', u'tokens', u'char_count',\n",
      "       u'word_count', u'urls', u'url_count', u'hashtags', u'hashtag_count',\n",
      "       u'mentions', u'mentions_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)\n",
    "print(popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add new column\n",
    "# TODO do with .loc, not copy\n",
    "\n",
    "char_count = []\n",
    "word_count = []\n",
    "urls = []\n",
    "hashtags = []\n",
    "mentions = []\n",
    "\n",
    "for i, row in not_popular_df.iterrows():\n",
    "    # Text and tokens\n",
    "    char_count.append(len(row['text']))\n",
    "    word_count.append(len(row['tokens']))\n",
    "    \n",
    "    # URLs\n",
    "    url_list = get_urls(row['tokens'])\n",
    "    urls.append(url_list)\n",
    "    url_count = len(url_list)\n",
    "    \n",
    "    # Hashtags\n",
    "    hashtag_list = get_hashtags(row['tokens'])\n",
    "    hashtags.append(hashtag_list)\n",
    "    hashtag_count = len(hashtag_list)\n",
    "    \n",
    "    # Mentions\n",
    "    mentions_list = get_mentions(row['tokens'])\n",
    "    mentions.append(mentions_list)\n",
    "    mentions_count = len(mentions_list)\n",
    "\n",
    "\n",
    "not_popular_df['char_count'] = char_count\n",
    "not_popular_df['word_count'] = word_count\n",
    "not_popular_df['urls'] = urls\n",
    "not_popular_df['url_count'] = url_count\n",
    "not_popular_df['hashtags'] = hashtags\n",
    "not_popular_df['hashtag_count'] = hashtag_count\n",
    "not_popular_df['mentions'] = hashtags\n",
    "not_popular_df['mentions_count'] = mentions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (18093, 12))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime', u'tokens', u'char_count',\n",
      "       u'word_count', u'urls', u'url_count', u'hashtags', u'hashtag_count',\n",
      "       u'mentions', u'mentions_count'],\n",
      "      dtype='object'))\n",
      "Index([u'text', u'rt_count', u'tweet_datetime', u'tokens', u'char_count',\n",
      "       u'word_count', u'urls', u'url_count', u'hashtags', u'hashtag_count',\n",
      "       u'mentions', u'mentions_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', not_popular_df.shape)\n",
    "print('Columns:', not_popular_df.columns)\n",
    "print(not_popular_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Extract from datetime\n",
    "\n",
    "Our times are not very diverse, so maybe not useful to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "\n",
    "This would be useful for their speech classifying exercise\n",
    "\n",
    "<http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popular_text_list = popular_df['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popular_vectorizer = CountVectorizer(min_df=1)\n",
    "popular_X = popular_vectorizer.fit_transform(popular_text_list)  # input is a list of strings, 1 per document\n",
    "popular_analyze = popular_vectorizer.build_analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'00',\n",
       " u'000',\n",
       " u'0000',\n",
       " u'00005000',\n",
       " u'004917641659029',\n",
       " u'00655',\n",
       " u'00am',\n",
       " u'00dd7yaxc5',\n",
       " u'00jywytzde',\n",
       " u'00m',\n",
       " u'00pm',\n",
       " u'00s',\n",
       " u'00te',\n",
       " u'00xsjjbhzb',\n",
       " u'01',\n",
       " u'011st1cbzg',\n",
       " u'01pm',\n",
       " u'01uelpmonr',\n",
       " u'01zapmvxzj',\n",
       " u'02',\n",
       " u'0209ahn',\n",
       " u'021xb0unig',\n",
       " u'02am',\n",
       " u'03',\n",
       " u'0300',\n",
       " u'034',\n",
       " u'038fyag08c',\n",
       " u'03oqllx4uy',\n",
       " u'03pm',\n",
       " u'03xijsddlm',\n",
       " u'04',\n",
       " u'04c1h7zobn',\n",
       " u'04k474lpa2',\n",
       " u'04pm',\n",
       " u'04pq2bbcww',\n",
       " u'05',\n",
       " u'0531elizabeth',\n",
       " u'058_powermoves',\n",
       " u'06',\n",
       " u'06dqbtkidm',\n",
       " u'06pbnrtycd',\n",
       " u'06s3lwobxt',\n",
       " u'06vdp3g3i7',\n",
       " u'07',\n",
       " u'0720shan',\n",
       " u'0731please',\n",
       " u'079',\n",
       " u'07dbh0lqxe',\n",
       " u'07lkntjyso',\n",
       " u'08',\n",
       " u'0811',\n",
       " u'082',\n",
       " u'0843',\n",
       " u'0892',\n",
       " u'08bydynpky',\n",
       " u'08h2',\n",
       " u'08vzuesa84',\n",
       " u'09',\n",
       " u'094',\n",
       " u'0992',\n",
       " u'09hhmblpyt',\n",
       " u'0agt0fiyz8',\n",
       " u'0am1a4gzxw',\n",
       " u'0aoqfaidom',\n",
       " u'0b0ljbc38b',\n",
       " u'0bca5peaza',\n",
       " u'0cl8pcjhiq',\n",
       " u'0cljhyamde',\n",
       " u'0e3ulqxfpp',\n",
       " u'0e4ozqvwql',\n",
       " u'0efai3ls1m',\n",
       " u'0euznfnidi',\n",
       " u'0ezyrtf5pt',\n",
       " u'0f0t3ixymb',\n",
       " u'0fbmzat00t',\n",
       " u'0fdij3ssy0',\n",
       " u'0fp5eas6jj',\n",
       " u'0fqaafgnze',\n",
       " u'0gcxhswjax',\n",
       " u'0gczhmem7w',\n",
       " u'0glcylo1hg',\n",
       " u'0gm4splzsx',\n",
       " u'0gr5smaffn',\n",
       " u'0h8qkbyfhp',\n",
       " u'0hbzemmdnf',\n",
       " u'0hcngj7hwo',\n",
       " u'0hjmfbxwex',\n",
       " u'0hzj3ysnvw',\n",
       " u'0i6dd4g8no',\n",
       " u'0imcagts09',\n",
       " u'0jqxz4rr1f',\n",
       " u'0js6ohharh',\n",
       " u'0k5vnayiiu',\n",
       " u'0kaiage',\n",
       " u'0kbnddzpiv',\n",
       " u'0kksdx7vzn',\n",
       " u'0l',\n",
       " u'0l2so5cagb',\n",
       " u'0l6218qofi',\n",
       " u'0lb6rutkwn',\n",
       " u'0litrbxkzh',\n",
       " u'0liviaiglesias',\n",
       " u'0lncotzspn',\n",
       " u'0mph',\n",
       " u'0mzyou9cam',\n",
       " u'0n',\n",
       " u'0naifkg7rb',\n",
       " u'0nijz5wy7z',\n",
       " u'0od3n3yq8v',\n",
       " u'0oi38ohzvp',\n",
       " u'0pbfwmuqui',\n",
       " u'0pbhqlzngx',\n",
       " u'0pc2qqxj1n',\n",
       " u'0ps9qk1o3h',\n",
       " u'0pymshp329',\n",
       " u'0q5n9bqr13',\n",
       " u'0q9vposzwz',\n",
       " u'0qjnj7p44z',\n",
       " u'0qkgnzyjpb',\n",
       " u'0qm5juqv0f',\n",
       " u'0qx008ynu8',\n",
       " u'0rkfvswp6u',\n",
       " u'0rw9gkgwa2',\n",
       " u'0scu8lmas1',\n",
       " u'0sofrmgwrg',\n",
       " u'0sq3hvp5wb',\n",
       " u'0t2yf3cj8y',\n",
       " u'0teuoajotc',\n",
       " u'0tlwxqhp5z',\n",
       " u'0tni1htl6c',\n",
       " u'0trasmvkh6',\n",
       " u'0uaiekwgyu',\n",
       " u'0upsouezzt',\n",
       " u'0uw4a3jl3q',\n",
       " u'0v6b6vomik',\n",
       " u'0vaejprw1u',\n",
       " u'0wusykdmwq',\n",
       " u'0x8ngbvyjx',\n",
       " u'0xyx9efuzj',\n",
       " u'0xzn5mwfqm',\n",
       " u'0ysbrvl8dr',\n",
       " u'0zjwvbct9g',\n",
       " u'0zkdaaw9by',\n",
       " u'0zyat7dleq',\n",
       " u'10',\n",
       " u'100',\n",
       " u'1000',\n",
       " u'100000000x',\n",
       " u'10000mph',\n",
       " u'1000blackgirlbooks',\n",
       " u'1001subapp',\n",
       " u'100million',\n",
       " u'100x',\n",
       " u'101',\n",
       " u'102',\n",
       " u'1027kiisfm',\n",
       " u'1028',\n",
       " u'103',\n",
       " u'1030',\n",
       " u'1037kissfm',\n",
       " u'1043freshradio',\n",
       " u'105',\n",
       " u'106',\n",
       " u'107',\n",
       " u'1075',\n",
       " u'108',\n",
       " u'10am',\n",
       " u'10billionwives',\n",
       " u'10bulan',\n",
       " u'10ep',\n",
       " u'10k',\n",
       " u'10m',\n",
       " u'10monthsold',\n",
       " u'10p',\n",
       " u'10pm',\n",
       " u'10qmy6qhcj',\n",
       " u'10th',\n",
       " u'10ve',\n",
       " u'10vpeople',\n",
       " u'10x',\n",
       " u'10x05',\n",
       " u'11',\n",
       " u'110',\n",
       " u'110nine',\n",
       " u'111bn',\n",
       " u'11293',\n",
       " u'1130',\n",
       " u'11316',\n",
       " u'1150',\n",
       " u'11536',\n",
       " u'115th',\n",
       " u'118',\n",
       " u'119',\n",
       " u'11alive',\n",
       " u'11alivenews',\n",
       " u'11am',\n",
       " u'11ampst',\n",
       " u'11bmuzik',\n",
       " u'11egmjoepq',\n",
       " u'11pm',\n",
       " u'11th',\n",
       " u'11wlpmowun',\n",
       " u'12',\n",
       " u'120',\n",
       " u'1202',\n",
       " u'1209',\n",
       " u'122nd',\n",
       " u'123456789',\n",
       " u'123lindgren',\n",
       " u'123taylorrenee',\n",
       " u'125',\n",
       " u'1250',\n",
       " u'125k',\n",
       " u'126',\n",
       " u'1271116',\n",
       " u'12gage_xii',\n",
       " u'12oz',\n",
       " u'12p8f5d8ab',\n",
       " u'12thmantim',\n",
       " u'12x11',\n",
       " u'12yearsahaitian',\n",
       " u'13',\n",
       " u'13200',\n",
       " u'134pounds',\n",
       " u'13d_alphaomega',\n",
       " u'13hours',\n",
       " u'13kaylaa',\n",
       " u'13ryfvmz9y',\n",
       " u'13talexx17',\n",
       " u'13th',\n",
       " u'14',\n",
       " u'140',\n",
       " u'140th',\n",
       " u'141',\n",
       " u'1437',\n",
       " u'1448',\n",
       " u'144p',\n",
       " u'146',\n",
       " u'1482',\n",
       " u'149',\n",
       " u'1492',\n",
       " u'14destinymarie',\n",
       " u'14gelly',\n",
       " u'14k',\n",
       " u'14r16urygx',\n",
       " u'14t',\n",
       " u'14th',\n",
       " u'14z9ctxajb',\n",
       " u'15',\n",
       " u'150',\n",
       " u'151',\n",
       " u'154',\n",
       " u'15mdicjhgp',\n",
       " u'15pm',\n",
       " u'15pts',\n",
       " u'15s',\n",
       " u'15sruy8qpa',\n",
       " u'15th',\n",
       " u'15wks1lup2',\n",
       " u'16',\n",
       " u'160',\n",
       " u'1600',\n",
       " u'160131',\n",
       " u'160206',\n",
       " u'160213',\n",
       " u'160214',\n",
       " u'160216',\n",
       " u'160217',\n",
       " u'1620',\n",
       " u'163',\n",
       " u'164',\n",
       " u'165',\n",
       " u'1668898678168789308',\n",
       " u'1680',\n",
       " u'1688',\n",
       " u'169',\n",
       " u'16gb',\n",
       " u'16lqjj0i0f',\n",
       " u'16p2cfkyed',\n",
       " u'16th',\n",
       " u'17',\n",
       " u'175f',\n",
       " u'176',\n",
       " u'176ahoops',\n",
       " u'178',\n",
       " u'17g9lssi00',\n",
       " u'17h9un2wxk',\n",
       " u'17ilxhvupt',\n",
       " u'17lbs',\n",
       " u'17m',\n",
       " u'17th',\n",
       " u'18',\n",
       " u'180',\n",
       " u'1801',\n",
       " u'1814',\n",
       " u'1819',\n",
       " u'182mo35vnb',\n",
       " u'183',\n",
       " u'185',\n",
       " u'1861',\n",
       " u'1869',\n",
       " u'187',\n",
       " u'1878',\n",
       " u'1880',\n",
       " u'1886',\n",
       " u'18h',\n",
       " u'18inarow',\n",
       " u'18k',\n",
       " u'18l0uq5xrs',\n",
       " u'18pm',\n",
       " u'18th',\n",
       " u'18years',\n",
       " u'18yr',\n",
       " u'19',\n",
       " u'190',\n",
       " u'1900',\n",
       " u'1910',\n",
       " u'1911',\n",
       " u'1912',\n",
       " u'1914',\n",
       " u'1916',\n",
       " u'1916sackvillest',\n",
       " u'1917',\n",
       " u'1922',\n",
       " u'1923',\n",
       " u'1928',\n",
       " u'1930',\n",
       " u'1932',\n",
       " u'1935',\n",
       " u'1939',\n",
       " u'194',\n",
       " u'1940',\n",
       " u'1942',\n",
       " u'1943',\n",
       " u'1944',\n",
       " u'195',\n",
       " u'1954',\n",
       " u'1956',\n",
       " u'1958',\n",
       " u'1959',\n",
       " u'196',\n",
       " u'1960',\n",
       " u'1960s',\n",
       " u'1962',\n",
       " u'1966',\n",
       " u'1967',\n",
       " u'1968',\n",
       " u'1969',\n",
       " u'1970',\n",
       " u'1970s',\n",
       " u'1975scabellos',\n",
       " u'1977',\n",
       " u'1979',\n",
       " u'1980',\n",
       " u'1980s',\n",
       " u'1983',\n",
       " u'1988',\n",
       " u'1989',\n",
       " u'1990',\n",
       " u'1991',\n",
       " u'1994',\n",
       " u'1994in',\n",
       " u'1996',\n",
       " u'1997',\n",
       " u'1999',\n",
       " u'19k3kesycq',\n",
       " u'19th',\n",
       " u'19tywebl1k',\n",
       " u'1__tunechi',\n",
       " u'1_brae',\n",
       " u'1_gramm_',\n",
       " u'1a',\n",
       " u'1ahgadot5g',\n",
       " u'1ayltvidwu',\n",
       " u'1b',\n",
       " u'1bnjrnoxtg',\n",
       " u'1bossnuwop',\n",
       " u'1cjbs6uhki',\n",
       " u'1cjdswp5zm',\n",
       " u'1ckoqybnqo',\n",
       " u'1corinthians',\n",
       " u'1corzwamkn',\n",
       " u'1cwlypkgba',\n",
       " u'1d',\n",
       " u'1d_overeighteen',\n",
       " u'1day',\n",
       " u'1dfthometownsos',\n",
       " u'1dih3neieo',\n",
       " u'1donchart',\n",
       " u'1dor',\n",
       " u'1droast',\n",
       " u'1ekvofpnjs',\n",
       " u'1eore02ovd',\n",
       " u'1epq0flabh',\n",
       " u'1ez3izsy6s',\n",
       " u'1facade',\n",
       " u'1fcbkicmtx',\n",
       " u'1ff2lov98n',\n",
       " u'1ffpzx8tdl',\n",
       " u'1fgdxbhhek',\n",
       " u'1fsb1m5lli',\n",
       " u'1fz0xpyf0r',\n",
       " u'1g086rdwi4',\n",
       " u'1gmjklxkyh',\n",
       " u'1grgldwlfj',\n",
       " u'1grqohmoc1',\n",
       " u'1h',\n",
       " u'1h4zuhgwep',\n",
       " u'1hdj5kxwjg',\n",
       " u'1hf8jniak5',\n",
       " u'1hyavkjtjq',\n",
       " u'1idbrv4qz4',\n",
       " u'1iliarurxj',\n",
       " u'1imahhuoq6',\n",
       " u'1imlpbkq9a',\n",
       " u'1jgs1nvsmh',\n",
       " u'1jjqsonujl',\n",
       " u'1jvipzreqe',\n",
       " u'1k',\n",
       " u'1kallenn',\n",
       " u'1kzaehoxtz',\n",
       " u'1l44bsl3sz',\n",
       " u'1l8wswbcdw',\n",
       " u'1ldliqbjjf',\n",
       " u'1lezw96kkt',\n",
       " u'1lkb79kutv',\n",
       " u'1lqi1uedh4',\n",
       " u'1ltlxsv2xu',\n",
       " u'1m',\n",
       " u'1me7mfsc2q',\n",
       " u'1mgupta',\n",
       " u'1millioncupsfar',\n",
       " u'1mlmsoftware',\n",
       " u'1mm',\n",
       " u'1mmlebwdsb',\n",
       " u'1mmpgh',\n",
       " u'1month',\n",
       " u'1mxi0nerih',\n",
       " u'1nbyslz28m',\n",
       " u'1ne',\n",
       " u'1nlzfgskjn',\n",
       " u'1nrwwyi92y',\n",
       " u'1o17juice',\n",
       " u'1ol9yxfblj',\n",
       " u'1oqon0yal8',\n",
       " u'1ox7rdaeoi',\n",
       " u'1p1mscvk7g',\n",
       " u'1phjfmcho7',\n",
       " u'1pm',\n",
       " u'1pmest',\n",
       " u'1q',\n",
       " u'1r1wpcujlc',\n",
       " u'1rujc2aj7c',\n",
       " u'1sebovqfza',\n",
       " u'1soaqzdnlr',\n",
       " u'1st',\n",
       " u'1stclassgirls',\n",
       " u'1swiftielove3',\n",
       " u'1t62dymjyy',\n",
       " u'1time',\n",
       " u'1toulfls0f',\n",
       " u'1truesaxon1968',\n",
       " u'1tzbbuab0o',\n",
       " u'1u2ppc9eye',\n",
       " u'1udalnh1gj',\n",
       " u'1ugb0tojsf',\n",
       " u'1umfcjy08l',\n",
       " u'1uwed3czjp',\n",
       " u'1v1',\n",
       " u'1vl3aijroh',\n",
       " u'1vote',\n",
       " u'1vufxboq66',\n",
       " u'1w6rbxskb0',\n",
       " u'1wmp1xfepq',\n",
       " u'1wrwvvokkf',\n",
       " u'1wuewtjvah',\n",
       " u'1x11',\n",
       " u'1x1ykvyehs',\n",
       " u'1xhvt30o2h',\n",
       " u'1xtra',\n",
       " u'1xunqapz86',\n",
       " u'1ycvqec8au',\n",
       " u'1yearold',\n",
       " u'1yid64zj0h',\n",
       " u'1yltj1jb4q',\n",
       " u'1ylyqpnwxb',\n",
       " u'1yvgmsjjfu',\n",
       " u'1ywsgcrko2',\n",
       " u'1yzncm9kv5',\n",
       " u'1z0admbtjo',\n",
       " u'1zfghazlor',\n",
       " u'1zmimvgowa',\n",
       " u'1zuobk8doy',\n",
       " u'20',\n",
       " u'200',\n",
       " u'2000',\n",
       " u'2001',\n",
       " u'2004',\n",
       " u'2005',\n",
       " u'2006',\n",
       " u'2008',\n",
       " u'2009',\n",
       " u'200gb',\n",
       " u'200k',\n",
       " u'200th',\n",
       " u'201',\n",
       " u'2010',\n",
       " u'2011',\n",
       " u'2012',\n",
       " u'2013',\n",
       " u'2014',\n",
       " u'2015',\n",
       " u'2016',\n",
       " u'2017',\n",
       " u'2018',\n",
       " u'2020',\n",
       " u'2024',\n",
       " u'203',\n",
       " u'203rshgaeo',\n",
       " u'206',\n",
       " u'20eheefmdg',\n",
       " u'20k',\n",
       " u'20p',\n",
       " u'20pounds',\n",
       " u'20s',\n",
       " u'20th',\n",
       " u'20utc',\n",
       " u'20yrs',\n",
       " u'20yvs72v8p',\n",
       " u'21',\n",
       " u'210',\n",
       " u'211',\n",
       " u'212kms',\n",
       " u'2147483647',\n",
       " u'215',\n",
       " u'216',\n",
       " u'2172',\n",
       " u'219',\n",
       " u'21s',\n",
       " u'21shzcpnvg',\n",
       " u'21ssb9a0oq',\n",
       " u'21st',\n",
       " u'22',\n",
       " u'220',\n",
       " u'2230s',\n",
       " u'225',\n",
       " u'226',\n",
       " u'228',\n",
       " u'22deepakg',\n",
       " u'22nd',\n",
       " u'22ptqutatz',\n",
       " u'22s0naansw',\n",
       " u'23',\n",
       " u'23andme',\n",
       " u'23pailqbc5',\n",
       " u'23pm',\n",
       " u'23rd',\n",
       " u'24',\n",
       " u'244',\n",
       " u'2469',\n",
       " u'246o',\n",
       " u'24853',\n",
       " u'24hrs',\n",
       " u'24k',\n",
       " u'24pm',\n",
       " u'24th',\n",
       " u'24x7',\n",
       " u'25',\n",
       " u'250',\n",
       " u'250k',\n",
       " u'251',\n",
       " u'251justin',\n",
       " u'253',\n",
       " u'256',\n",
       " u'257',\n",
       " u'258mafia',\n",
       " u'25am',\n",
       " u'25bdsbwuzl',\n",
       " u'25k',\n",
       " u'25p8dofgkr',\n",
       " u'25th',\n",
       " u'26',\n",
       " u'265',\n",
       " u'2667861',\n",
       " u'26feb',\n",
       " u'26hyfmragw',\n",
       " u'26n4xouhzt',\n",
       " u'26shirts',\n",
       " u'26th',\n",
       " u'26yearsofagneepath',\n",
       " u'27',\n",
       " u'270',\n",
       " u'2754',\n",
       " u'276',\n",
       " u'27pm',\n",
       " u'27ppg',\n",
       " u'27th',\n",
       " u'28',\n",
       " u'2800stunnaman',\n",
       " u'285talife',\n",
       " u'287',\n",
       " u'28809th',\n",
       " u'289f',\n",
       " u'28th',\n",
       " u'29',\n",
       " u'2982',\n",
       " u'29kwnopyaw',\n",
       " u'29thofjune',\n",
       " u'2__flee',\n",
       " u'2a',\n",
       " u'2ad63x8esd',\n",
       " u'2ag4wpkf7a',\n",
       " u'2am',\n",
       " u'2apdbb1t52',\n",
       " u'2apg8as6ba',\n",
       " u'2aptby9a7d',\n",
       " u'2bysohan8v',\n",
       " u'2curbchainz',\n",
       " u'2day',\n",
       " u'2dd1ndepsi',\n",
       " u'2dffondmu1',\n",
       " u'2dukegirl21',\n",
       " u'2dvrpwsmcs',\n",
       " u'2dyzu1gp5c',\n",
       " u'2ehxu5clbt',\n",
       " u'2en7oyphhi',\n",
       " u'2erzggf6mj',\n",
       " u'2f5ldbzszc',\n",
       " u'2fdbwarrms',\n",
       " u'2fnrmyc9qk',\n",
       " u'2frmiygubn',\n",
       " u'2fuznuzs4q',\n",
       " u'2ga0wtozja',\n",
       " u'2gb',\n",
       " u'2gether',\n",
       " u'2gzud4dr87',\n",
       " u'2hgif15lyn',\n",
       " u'2hr',\n",
       " u'2ieix8fszw',\n",
       " u'2ih7wtfdbd',\n",
       " u'2iuk4fekvm',\n",
       " u'2izll8xyw2',\n",
       " u'2j5fi9ajqa',\n",
       " u'2juf2vxnua',\n",
       " u'2jwuctanyo',\n",
       " u'2k',\n",
       " u'2k0es8peh4',\n",
       " u'2k5wzwbujh',\n",
       " u'2kfreeagent',\n",
       " u'2knawwaozi',\n",
       " u'2kpl7bqxem',\n",
       " u'2lavutp6we',\n",
       " u'2leggeddog',\n",
       " u'2lt7jfczfd',\n",
       " u'2m5ub8guuf',\n",
       " u'2m8duokzpl',\n",
       " u'2miljbsmzj',\n",
       " u'2minache',\n",
       " u'2mmgv6fg8o',\n",
       " u'2mpd5gnr90',\n",
       " u'2nd',\n",
       " u'2ndaryela',\n",
       " u'2ne0unelzu',\n",
       " u'2ne1',\n",
       " u'2ngqdooxue',\n",
       " u'2night',\n",
       " u'2nkbgzdioo',\n",
       " u'2nl1_',\n",
       " u'2npoqrfag8',\n",
       " u'2ny',\n",
       " u'2nyt',\n",
       " u'2oncrew',\n",
       " u'2p2kgtvytc',\n",
       " u'2p6loseusg',\n",
       " u'2pac',\n",
       " u'2packs',\n",
       " u'2phone',\n",
       " u'2pljojytwz',\n",
       " u'2pm',\n",
       " u'2potxaa9pz',\n",
       " u'2qbviaunnf',\n",
       " u'2qdrkmicc0',\n",
       " u'2qecpx0c5w',\n",
       " u'2rfe3fchlz',\n",
       " u'2s',\n",
       " u'2sassy4321',\n",
       " u'2sg1gohl3q',\n",
       " u'2sn7lztpm2',\n",
       " u'2tautxaudu',\n",
       " u'2tbdwznbbs',\n",
       " u'2tk5gdzi3u',\n",
       " u'2tqdqqc1sm',\n",
       " u'2tzpdspyh8',\n",
       " u'2uns0lnnl6',\n",
       " u'2usqhpiz96',\n",
       " u'2vz5sfkbpu',\n",
       " u'2wckkc19rc',\n",
       " u'2wd',\n",
       " u'2wis0kmyse',\n",
       " u'2wm9pqdd9r',\n",
       " u'2wmnhpjnyv',\n",
       " u'2wq2u9g55a',\n",
       " u'2x',\n",
       " u'2xierp3p2l',\n",
       " u'2xq4kiwxbh',\n",
       " u'2xqsfztu5d',\n",
       " u'2xw8ftz7va',\n",
       " u'2xwinners',\n",
       " u'2xybjc8wxh',\n",
       " u'2ylrnzhirk',\n",
       " u'2yvwi9jhbo',\n",
       " u'2zb9y1eyjw',\n",
       " u'30',\n",
       " u'300',\n",
       " u'3000',\n",
       " u'3005bts',\n",
       " u'300fge_vello024',\n",
       " u'30200',\n",
       " u'3036',\n",
       " u'305',\n",
       " u'307',\n",
       " u'3085',\n",
       " u'30am',\n",
       " u'30m',\n",
       " u'30pm',\n",
       " u'30s',\n",
       " u'30t',\n",
       " u'30th',\n",
       " u'31',\n",
       " u'310',\n",
       " u'311',\n",
       " u'313',\n",
       " u'31am',\n",
       " u'31k',\n",
       " u'32',\n",
       " u'320',\n",
       " u'3200',\n",
       " u'323',\n",
       " u'3242',\n",
       " u'327',\n",
       " u'32_carsondye',\n",
       " u'32gb',\n",
       " u'32jmke6vnm',\n",
       " u'32oz',\n",
       " u'32wd9ivim9',\n",
       " u'33',\n",
       " u'330',\n",
       " u'333',\n",
       " u'3333',\n",
       " u'33333',\n",
       " u'3382a9yyfu',\n",
       " u'33am',\n",
       " u'34',\n",
       " u'342',\n",
       " u'343',\n",
       " u'344',\n",
       " u'345',\n",
       " u'34am',\n",
       " u'34habzvgjy',\n",
       " u'34iumon9cz',\n",
       " u'35',\n",
       " u'3500',\n",
       " u'35o',\n",
       " u'35s',\n",
       " u'35vdymgl9b',\n",
       " u'36',\n",
       " u'360',\n",
       " u'360ffb',\n",
       " u'362',\n",
       " u'364',\n",
       " u'365',\n",
       " u'3655',\n",
       " u'369',\n",
       " u'36am',\n",
       " u'36dedx9qii',\n",
       " u'36q5qkxkbl',\n",
       " u'37',\n",
       " u'370z',\n",
       " u'38',\n",
       " u'380',\n",
       " u'383',\n",
       " u'38egskeb7r',\n",
       " u'38fqivtkrp',\n",
       " u'39',\n",
       " u'393sj',\n",
       " u'3a',\n",
       " u'3aajv5qxso',\n",
       " u'3agpygun6l',\n",
       " u'3akgpvkde7',\n",
       " u'3am',\n",
       " u'3b2xd6igtk',\n",
       " u'3bafhwxfn0',\n",
       " u'3bmt36mkcd',\n",
       " u'3braqdhpwu',\n",
       " u'3bzyopwgn8',\n",
       " u'3c7kwkbdc8',\n",
       " u'3c7m5njkia',\n",
       " u'3carrotsindy',\n",
       " u'3catstn',\n",
       " u'3ceo6tgr0e',\n",
       " u'3cqer90vlv',\n",
       " u'3cromeqbmh',\n",
       " u'3d',\n",
       " u'3dckkndbil',\n",
       " u'3dcoaches',\n",
       " u'3dp',\n",
       " u'3dprinting',\n",
       " u'3drbrphtwr',\n",
       " u'3ds0nixz',\n",
       " u'3eblo8wmjf',\n",
       " u'3eisrdum',\n",
       " u'3ernww8gav',\n",
       " u'3evjfytp79',\n",
       " u'3f2ifnn71c',\n",
       " u'3faqdgobms',\n",
       " u'3fclothing',\n",
       " u'3fcnugiwju',\n",
       " u'3fjm0h2het',\n",
       " u'3fmhzxanhc',\n",
       " u'3fs3ibdysf',\n",
       " u'3ftugwl8nn',\n",
       " u'3g',\n",
       " u'3g5dsitigr',\n",
       " u'3gbsblhmww',\n",
       " u'3gi3vgvtt8',\n",
       " u'3gltiu2unw',\n",
       " u'3go3dvfbji',\n",
       " u'3grdpt4z5i',\n",
       " u'3h',\n",
       " u'3hao2ewrx5',\n",
       " u'3hd6qv7wh0',\n",
       " u'3hnkepicwo',\n",
       " u'3hobv3pkhc',\n",
       " u'3huuupr20k',\n",
       " u'3ijxrhp5i6',\n",
       " u'3ipwgtkhkj',\n",
       " u'3iqbndfjjw',\n",
       " u'3jedlruxz7',\n",
       " u'3jeepdywqt',\n",
       " u'3jeicnsomb',\n",
       " u'3jo0wwrgtq',\n",
       " u'3jtgyu9f1q',\n",
       " u'3jwutamjpb',\n",
       " u'3k',\n",
       " u'3k3t83ueof',\n",
       " u'3k77ouzvkq',\n",
       " u'3kea8phmmh',\n",
       " u'3keebzts7f',\n",
       " u'3kvtvcl8zv',\n",
       " u'3lbs',\n",
       " u'3lmjwcu8o7',\n",
       " u'3lpxmxuvl9',\n",
       " u'3m',\n",
       " u'3mibcafnba',\n",
       " u'3mqq1c6hfu',\n",
       " u'3mxkqrrfto',\n",
       " u'3nrdoqohrx',\n",
       " u'3nvewmowhc',\n",
       " u'3oabi8lb3b',\n",
       " u'3ocqbwgucy',\n",
       " u'3omiobkvix',\n",
       " u'3owzxv8kta',\n",
       " u'3p1lnu9xfh',\n",
       " u'3pfbavclvw',\n",
       " u'3pj0b8tw84',\n",
       " u'3pl3hjapkc',\n",
       " u'3plmo2m0u1',\n",
       " u'3pm',\n",
       " u'3pqyftw1gw',\n",
       " u'3ptr',\n",
       " u'3pw2oup01r',\n",
       " u'3qnfxdo4to',\n",
       " u'3qqlicgb7k',\n",
       " u'3qw2oossqc',\n",
       " u'3qxrzgeksd',\n",
       " u'3r274naknb',\n",
       " u'3rd',\n",
       " u'3rdandmiles',\n",
       " u'3rj1j2tqdp',\n",
       " u'3rovb8qudf',\n",
       " u'3rrms9votw',\n",
       " u'3rzdolqti5',\n",
       " u'3s',\n",
       " u'3se51hn9bv',\n",
       " u'3some',\n",
       " u'3t9tiqnlxl',\n",
       " u'3tdfvpxuwf',\n",
       " u'3tuavusoj6',\n",
       " u'3tv',\n",
       " u'3v3',\n",
       " u'3v8m7a07yr',\n",
       " u'3vu3e96vhs',\n",
       " u'3vvporzmcu',\n",
       " u'3wor6phibb',\n",
       " u'3wrnfsuy1n',\n",
       " u'3wybsw0pxa',\n",
       " u'3wztkpsqw5',\n",
       " u'3x',\n",
       " u'3xbf4onsfp',\n",
       " u'3xr9hrajlv',\n",
       " u'3yrs',\n",
       " u'3yspfdfbya',\n",
       " u'3ytm5ozngv',\n",
       " u'3yv2xqr2mf',\n",
       " u'3z5scxzvcp',\n",
       " u'3z6xpj',\n",
       " u'3zkmklxfwp',\n",
       " u'3zlywlowmr',\n",
       " u'3zqdhquvbq',\n",
       " u'3zsggoieps',\n",
       " u'3zuxoacpkv',\n",
       " u'40',\n",
       " u'400',\n",
       " u'4000',\n",
       " u'400000',\n",
       " u'403b',\n",
       " u'40iznraotp',\n",
       " u'40m',\n",
       " u'40th',\n",
       " u'40z',\n",
       " u'41',\n",
       " u'411wshgwll',\n",
       " u'4122317872',\n",
       " u'4154',\n",
       " u'416leftyy',\n",
       " u'41esks9o1u',\n",
       " u'41fzrewggu',\n",
       " u'41yjkoyayt',\n",
       " u'42',\n",
       " u'429ojaltoc',\n",
       " u'42a1539web',\n",
       " u'42am',\n",
       " u'42nd',\n",
       " u'42r83lc3am',\n",
       " u'43',\n",
       " u'430',\n",
       " u'431sa0xftg',\n",
       " u'43324fan',\n",
       " u'435',\n",
       " u'437',\n",
       " u'438',\n",
       " u'43y8cajwmc',\n",
       " u'44',\n",
       " u'443chat',\n",
       " u'447faqtgq0',\n",
       " u'44izsjvnxf',\n",
       " u'44jqa0nzkb',\n",
       " u'44kinghenry',\n",
       " u'44m505ac1x',\n",
       " u'44plggqijv',\n",
       " u'44wkljjpsu',\n",
       " u'45',\n",
       " u'452',\n",
       " u'457',\n",
       " u'45c8pezbjl',\n",
       " u'45crh6endg',\n",
       " u'45i4ptb0bg',\n",
       " u'45kgqu7gt7',\n",
       " u'45m',\n",
       " u'45nikrl9nd',\n",
       " u'46',\n",
       " u'469wfzlkwc',\n",
       " u'46am',\n",
       " u'46c',\n",
       " u'46eti2suhk',\n",
       " u'46nqfusbe4',\n",
       " u'46usqvfssn',\n",
       " u'47',\n",
       " u'473',\n",
       " u'4734',\n",
       " u'47lunjtdhy',\n",
       " u'48',\n",
       " u'480',\n",
       " u'4800',\n",
       " u'485',\n",
       " u'4859',\n",
       " u'48c',\n",
       " u'48je9mqoy8',\n",
       " u'49',\n",
       " u'492148bb124c4bd',\n",
       " u'4924',\n",
       " u'49ers',\n",
       " u'49g67ytlwv',\n",
       " u'49vozx5euu',\n",
       " u'49x365',\n",
       " u'49zin78mda',\n",
       " u'4a',\n",
       " u'4a2aruyk',\n",
       " u'4afee',\n",
       " u'4am',\n",
       " u'4anniereilly',\n",
       " u'4aujvhn3rz',\n",
       " u'4azpwueo7g',\n",
       " u'4b',\n",
       " u'4b78kunbyb',\n",
       " u'4bgqnykeqd',\n",
       " u'4bmk1kx1kk',\n",
       " u'4bn',\n",
       " u'4bultcox6w',\n",
       " u'4burundians',\n",
       " ...]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39880"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(popular_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# then, figure out how to add each of these sparse vectors to the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 32399)\t1\n",
      "  (0, 32612)\t1\n",
      "  (0, 25111)\t1\n",
      "  (0, 25247)\t1\n",
      "  (0, 29129)\t1\n",
      "  (1, 12089)\t1\n",
      "  (1, 29274)\t1\n",
      "  (1, 28429)\t1\n",
      "  (1, 33195)\t1\n",
      "  (1, 35351)\t1\n",
      "  (1, 8485)\t1\n",
      "  (1, 17723)\t1\n",
      "  (1, 37587)\t1\n",
      "  (1, 3378)\t1\n",
      "  (1, 13717)\t1\n",
      "  (1, 3671)\t1\n",
      "  (1, 27405)\t1\n",
      "  (1, 34562)\t1\n",
      "  (1, 39323)\t1\n",
      "  (1, 29056)\t1\n",
      "  (1, 26617)\t1\n",
      "  (1, 37686)\t1\n",
      "  (1, 25703)\t1\n",
      "  (1, 16605)\t1\n",
      "  (2, 18275)\t1\n",
      "  :\t:\n",
      "  (17679, 20622)\t1\n",
      "  (17679, 9285)\t1\n",
      "  (17679, 14643)\t1\n",
      "  (17679, 21989)\t1\n",
      "  (17679, 13377)\t1\n",
      "  (17680, 34562)\t1\n",
      "  (17680, 16605)\t1\n",
      "  (17680, 8326)\t1\n",
      "  (17680, 25928)\t1\n",
      "  (17680, 26604)\t1\n",
      "  (17680, 37020)\t1\n",
      "  (17680, 5082)\t1\n",
      "  (17680, 20617)\t1\n",
      "  (17680, 29603)\t1\n",
      "  (17680, 38417)\t1\n",
      "  (17680, 32132)\t1\n",
      "  (17680, 28658)\t1\n",
      "  (17680, 28670)\t1\n",
      "  (17680, 39296)\t1\n",
      "  (17680, 25702)\t1\n",
      "  (17681, 17834)\t1\n",
      "  (17681, 37687)\t1\n",
      "  (17681, 22834)\t1\n",
      "  (17681, 38244)\t1\n",
      "  (17681, 7003)\t1\n"
     ]
    }
   ],
   "source": [
    "print(popular_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "popular_bow_array = popular_vectorizer.fit_transform(popular_text_list).toarray()  # is this what we want?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_vectorizer.transform(['and an example tweet']).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17682L, 39880L)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_bow_array.shape  # first value is the # of rows (ie, tweets) in the DataFrame; second is # of vocab words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ..., \n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_bow_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# How to add this vector as a vector?\n",
    "# popular_vectorizer.transform(['and an example tweet']).toarray()\n",
    "\n",
    "#! Read this: https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words\n",
    "\n",
    "#bow_vector = popular_vectorizer.transform([row['text']]).toarray()\n",
    "#bow_vectors.append(bow_vector)\n",
    "#popular_df['bow_vector'] = bow_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! an alternative is to ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Think about how to put into feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write DataFrame to csv\n",
    "\n",
    "The next notebook will pick up from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popular_df.to_csv('feature_tables/popular_tweet_features.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_popular_df.to_csv('feature_tables/not_popular_tweet_features.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
