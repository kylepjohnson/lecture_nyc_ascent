{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open with csv module, iterate row-by-row\n",
    "with open('tweets/tweets_popular.csv', 'rb') as file_open:\n",
    "    popular_csv = csv.reader(file_open, delimiter='|')\n",
    "    for row in popular_csv:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open with Pandas, load into DataFrame\n",
    "\n",
    "# TODO: Parse dates correctly; this is close but not working\n",
    "date_parser = lambda x: pandas.datetime.strptime(x, '%a %b %d %H:%M:%S +z %Y')  # Mon Feb 15 20:44:33 +0000 2016\n",
    "\n",
    "popular_df = pandas.read_csv('tweets/tweets_popular.csv', \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False, \n",
    "                             parse_dates=True,\n",
    "                             date_parser=date_parser\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (3028, 3))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our data\n",
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column\n",
      "0    @CringeLMAO: Easy there m8 https://t.co/dnF3Wq...\n",
      "1    @AustinMahone: Just posted a photo https://t.c...\n",
      "2    @Ashton5SOS: Some days I drink way to much cof...\n",
      "3    @lailamuhammad: When you nail that #Beyonc   m...\n",
      "Name: text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Text column')\n",
    "print(popular_df['text'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet count\n",
      "0     2084\n",
      "1     1059\n",
      "2    24121\n",
      "3      801\n",
      "Name: rt_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Retweet count')\n",
    "print(popular_df['rt_count'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date-time\n",
      "0    Mon Feb 15 20:44:33 +0000 2016\n",
      "1    Mon Feb 15 20:44:33 +0000 2016\n",
      "2    Mon Feb 15 20:44:33 +0000 2016\n",
      "3    Mon Feb 15 20:44:33 +0000 2016\n",
      "Name: tweet_datetime, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Date-time')\n",
    "print(popular_df['tweet_datetime'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the parsed date-time\n",
    "# TODO: Try to parse this right\n",
    "dt = popular_df['tweet_datetime'][0]\n",
    "print(type(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (11328, 3))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "# Do the same for unpopular data\n",
    "not_popular_df = pandas.read_csv('tweets/tweets_not_popular.csv', \n",
    "                                 delimiter='|', \n",
    "                                 error_bad_lines=False, \n",
    "                                 warn_bad_lines=False,\n",
    "                                 parse_dates=True,\n",
    "                                 date_parser=date_parser)\n",
    "\n",
    "# Let's inspect our data\n",
    "print('Shape:', not_popular_df.shape)\n",
    "print('Columns:', not_popular_df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape before', (3028, 3))\n",
      "('Shape after', (2919, 3))\n"
     ]
    }
   ],
   "source": [
    "print('Shape before', popular_df.shape)\n",
    "popular_df = popular_df.drop_duplicates()\n",
    "print('Shape after', popular_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape before', (11328, 3))\n",
      "('Shape after', (11328, 3))\n"
     ]
    }
   ],
   "source": [
    "print('Shape before', not_popular_df.shape)\n",
    "popular_df = not_popular_df.drop_duplicates()\n",
    "print('Shape after', not_popular_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other cleanup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "\n",
    "Show plain function, maybe NLTK too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer\n",
    "\n",
    "def tokenize_words(input_string):\n",
    "    \"\"\"Take a string, return a list of \n",
    "    strings broken on whitespace, but do \n",
    "    not break @mentions and URLs.\n",
    "    \"\"\"\n",
    "    punctuation = [',', '!', '\"', '. ', ': ']\n",
    "    for char in punctuation:\n",
    "        input_string = input_string.replace(char, ' ')\n",
    "    \n",
    "    return [w for w in input_string.split(' ') if w]  # rm empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@taylorcaniff',\n",
       " 'Never',\n",
       " 'mind',\n",
       " \"I'm\",\n",
       " 'snowed',\n",
       " 'in',\n",
       " 'again',\n",
       " 'I',\n",
       " \"can't\",\n",
       " 'quit',\n",
       " 'laughing']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_tweet = \"@taylorcaniff: Never mind I'm snowed in again I can't quit laughing\"\n",
    "tokenize_words(a_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@CuteEmergency', \"I'm\", 'okay', 'https://t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See @users and http: not split\n",
    "another_tweet = \"\"\"@CuteEmergency: \"I'm okay!\" https://t.co/TWMwjG03Fd\"\"\"\n",
    "tokenize_words(another_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using the Python re library \n",
    "def tokenize_words_regex(input_string):\n",
    "    \"\"\"Tokenize input string with re library,\n",
    "    return list of strings.\"\"\"\n",
    "    tokenization_regex = re.compile(r\"[\\w']+|[.,!?;]\")\n",
    "    return tokenization_regex.findall(input_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['CuteEmergency', \"I'm\", 'okay', '!', 'https', 't', '.', 'co', 'TWMwjG03Fd']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_words_regex(another_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@',\n",
       " 'CuteEmergency',\n",
       " ':',\n",
       " '\"',\n",
       " 'I',\n",
       " \"'m\",\n",
       " 'okay',\n",
       " '!',\n",
       " '\"',\n",
       " 'https',\n",
       " ':',\n",
       " '//t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK has one, too but still breaks up what we need,\n",
    "# we'll skip for this exercise\n",
    "from nltk.tokenize.punkt import PunktLanguageVars\n",
    "\n",
    "nltk_tokenizer = PunktLanguageVars()\n",
    "nltk_tokenizer.word_tokenize(another_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "# TODO do with .loc, not copy\n",
    "\n",
    "tokens = []  # list of strings\n",
    "\n",
    "for i, row in popular_df.iterrows():\n",
    "    tokens.append(tokenize_words(row['text'])) \n",
    "\n",
    "popular_df['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (11113, 4))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime', u'tokens'], dtype='object'))\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'text', u'rt_count', u'tweet_datetime', u'tokens'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "popular_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named entity recognition (NER)\n",
    "\n",
    "Show NLTK code, not for feature table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting text\n",
    "\n",
    "* count chars\n",
    "* count words\n",
    "* links\n",
    "* count links\n",
    "* #hashtags\n",
    "* count #hashtags\n",
    "* @mentions\n",
    "* count @mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `http(s)://`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('http')]`\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('http'):\n",
    "            urls.append(word)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `#`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('#')]`\n",
    "    \"\"\"\n",
    "    hashtags = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('#'):\n",
    "            hashtags.append(word)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `@`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('@')]`\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('@'):\n",
    "            mentions.append(word)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:32: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:35: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:36: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:37: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "C:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\ipykernel\\__main__.py:38: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# Add new column\n",
    "# TODO do with .loc, not copy\n",
    "\n",
    "char_count = []\n",
    "word_count = []\n",
    "urls = []\n",
    "hashtags = []\n",
    "mentions = []\n",
    "\n",
    "for i, row in popular_df.iterrows():\n",
    "    # Text and tokens\n",
    "    char_count.append(len(row['text']))\n",
    "    word_count.append(len(row['tokens']))\n",
    "    \n",
    "    # URLs\n",
    "    url_list = get_urls(row['tokens'])\n",
    "    urls.append(url_list)\n",
    "    url_count = len(url_list)\n",
    "    \n",
    "    # Hashtags\n",
    "    hashtag_list = get_hashtags(row['tokens'])\n",
    "    hashtags.append(hashtag_list)\n",
    "    hashtag_count = len(hashtag_list)\n",
    "    \n",
    "    # Mentions\n",
    "    mentions_list = get_mentions(row['tokens'])\n",
    "    mentions.append(mentions_list)\n",
    "    mentions_count = len(mentions_list)\n",
    "\n",
    "\n",
    "popular_df['char_count'] = char_count\n",
    "popular_df['word_count'] = word_count\n",
    "popular_df['urls'] = urls\n",
    "popular_df['url_count'] = url_count\n",
    "popular_df['hashtags'] = hashtags\n",
    "popular_df['hashtag_count'] = hashtag_count\n",
    "popular_df['mentions'] = hashtags\n",
    "popular_df['mentions_count'] = mentions_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Shape:', (11113, 12))\n",
      "('Columns:', Index([u'text', u'rt_count', u'tweet_datetime', u'tokens', u'char_count',\n",
      "       u'word_count', u'urls', u'url_count', u'hashtags', u'hashtag_count',\n",
      "       u'mentions', u'mentions_count'],\n",
      "      dtype='object'))\n",
      "Index([u'text', u'rt_count', u'tweet_datetime', u'tokens', u'char_count',\n",
      "       u'word_count', u'urls', u'url_count', u'hashtags', u'hashtag_count',\n",
      "       u'mentions', u'mentions_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)\n",
    "print(popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for i, row in popular_df.iterrows():\n",
    "#    print(row)\n",
    "#    input()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting from datetime?\n",
    "\n",
    "Problem here is our times are not very diverse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words?\n",
    "\n",
    "This would be useful for their speech classifying exercise\n",
    "\n",
    "<http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Think about how to put into feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write DataFrame to csv\n",
    "\n",
    "The next notebook will pick up from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "popular_df.to_csv('feature_tables/popular_tweet_features.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_popular_df.to_csv('feature_tables/not_popular_tweet_features.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
