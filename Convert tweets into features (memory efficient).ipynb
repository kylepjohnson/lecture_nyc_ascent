{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import re\n",
    "\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open with csv module, iterate row-by-row\n",
    "with open('tweets/tweets_popular.csv', 'rb') as file_open:\n",
    "    popular_csv = csv.reader(file_open, delimiter='|')\n",
    "    for row in popular_csv:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open with Pandas, load into DataFrame\n",
    "\n",
    "# TODO: Parse dates correctly; this is close but not working\n",
    "date_parser = lambda x: pandas.datetime.strptime(x, '%a %b %d %H:%M:%S +z %Y')  # Mon Feb 15 20:44:33 +0000 2016\n",
    "\n",
    "popular_df = pandas.read_csv('tweets/tweets_popular.csv', \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False, \n",
    "                             parse_dates=True,\n",
    "                             date_parser=date_parser\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4679, 3)\n",
      "Columns: Index([u'_text', u'_rt_count', u'_tweet_datetime'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our data\n",
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column\n",
      "0    @CringeLMAO: Easy there m8 https://t.co/dnF3Wq...\n",
      "1    @AustinMahone: Just posted a photo https://t.c...\n",
      "2    @Ashton5SOS: Some days I drink way to much cof...\n",
      "3    @lailamuhammad: When you nail that #Beyonc   m...\n",
      "Name: _text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Text column')\n",
    "print(popular_df['_text'][:4])  # the underscores will be apparent later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet count\n",
      "0     2084\n",
      "1     1059\n",
      "2    24121\n",
      "3      801\n",
      "Name: _rt_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Retweet count')\n",
    "print(popular_df['_rt_count'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date-time\n",
      "0    Mon Feb 15 20:44:33 +0000 2016\n",
      "1    Mon Feb 15 20:44:33 +0000 2016\n",
      "2    Mon Feb 15 20:44:33 +0000 2016\n",
      "3    Mon Feb 15 20:44:33 +0000 2016\n",
      "Name: _tweet_datetime, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Date-time')\n",
    "print(popular_df['_tweet_datetime'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the parsed date-time\n",
    "# TODO: Try to parse this right\n",
    "dt = popular_df['_tweet_datetime'][0]\n",
    "print(type(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's do this again, but with a function\n",
    "del popular_df  # rm large object from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_df(csv_file):\n",
    "    \"\"\"Open csv, return Pandas DataFrame.\"\"\"\n",
    "    dataframe = pandas.read_csv(csv_file, \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False, \n",
    "                             parse_dates=True,\n",
    "                             date_parser=date_parser\n",
    "                            )\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_popular = csv_to_df('tweets/tweets_popular.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (4679, 3)\n",
      "Shape after: (4679, 3)\n"
     ]
    }
   ],
   "source": [
    "# rows, columns\n",
    "print('Shape before:', dataframe_popular.shape)\n",
    "\n",
    "popular_df = dataframe_popular.drop_duplicates()\n",
    "print('Shape after:', dataframe_popular.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other cleanup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Todo\n",
    "\n",
    "# case, space, some punctuation, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "\n",
    "Show plain function, maybe NLTK too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer\n",
    "\n",
    "def tokenize_words(input_string):\n",
    "    \"\"\"Take a string, return a list of \n",
    "    strings broken on whitespace, but do \n",
    "    not break @mentions and URLs.\n",
    "    \"\"\"\n",
    "    punctuation = [',', '!', '\"', '. ', ': ']\n",
    "    for char in punctuation:\n",
    "        input_string = input_string.replace(char, ' ')\n",
    "    \n",
    "    return [w for w in input_string.split(' ') if w]  # rm empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@CuteEmergency', \"I'm\", 'okay', 'https://t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See @users and http: not split\n",
    "a_tweet = \"\"\"@CuteEmergency: \"I'm okay!\" https://t.co/TWMwjG03Fd\"\"\"\n",
    "tokenize_words(a_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting text\n",
    "\n",
    "* count chars\n",
    "* count words\n",
    "* links\n",
    "* count links\n",
    "* #hashtags\n",
    "* count #hashtags\n",
    "* @mentions\n",
    "* count @mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `http(s)://`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('http')]`\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('http'):\n",
    "            urls.append(word)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `#`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('#')]`\n",
    "    \"\"\"\n",
    "    hashtags = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('#'):\n",
    "            hashtags.append(word)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `@`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('@')]`\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('@'):\n",
    "            mentions.append(word)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features_to_df(dataframe):\n",
    "    \"\"\"Take DataFrame of tweets, extract some specific \n",
    "    features and add to returned DataFrame.\n",
    "    \"\"\"\n",
    "    #tokens = []  # list of strings\n",
    "    char_count = []\n",
    "    word_count = []\n",
    "    urls = []\n",
    "    hashtags = []\n",
    "    mentions = []\n",
    "\n",
    "    for i, row in dataframe.iterrows():\n",
    "        \n",
    "        # Text and tokens\n",
    "        tokens = tokenize_words(row['_text'])\n",
    "        char_count.append(len(row['_text']))\n",
    "        word_count.append(len(tokens))\n",
    "\n",
    "        # URLs\n",
    "        url_list = get_urls(tokens)\n",
    "        urls.append(url_list)\n",
    "        url_count = len(url_list)\n",
    "\n",
    "        # Hashtags\n",
    "        hashtag_list = get_hashtags(tokens)\n",
    "        hashtags.append(hashtag_list)\n",
    "        hashtag_count = len(hashtag_list)\n",
    "\n",
    "        # Mentions\n",
    "        mentions_list = get_mentions(tokens)\n",
    "        mentions.append(mentions_list)\n",
    "        mentions_count = len(mentions_list)\n",
    "\n",
    "    #dataframe['_tokens'] = tokens\n",
    "    dataframe['_char_count'] = char_count\n",
    "    dataframe['_word_count'] = word_count\n",
    "    dataframe['_urls'] = urls\n",
    "    dataframe['_url_count'] = url_count\n",
    "    dataframe['_hashtags'] = hashtags\n",
    "    dataframe['_hashtag_count'] = hashtag_count\n",
    "    dataframe['_mentions'] = hashtags\n",
    "    dataframe['_mentions_count'] = mentions_count\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_popular = add_features_to_df(dataframe_popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4679, 11)\n",
      "Columns: Index([u'_text', u'_rt_count', u'_tweet_datetime', u'_char_count',\n",
      "       u'_word_count', u'_urls', u'_url_count', u'_hashtags',\n",
      "       u'_hashtag_count', u'_mentions', u'_mentions_count'],\n",
      "      dtype='object')\n",
      "Index([u'_text', u'_rt_count', u'_tweet_datetime', u'_char_count',\n",
      "       u'_word_count', u'_urls', u'_url_count', u'_hashtags',\n",
      "       u'_hashtag_count', u'_mentions', u'_mentions_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', dataframe_popular.shape)\n",
    "print('Columns:', dataframe_popular.columns)\n",
    "print(dataframe_popular.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Extract from datetime\n",
    "\n",
    "Our times are not very diverse, so will not useful for the feature table in this data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Named entity recognition (NER)\n",
    "\n",
    "Maybe show NLTK code, but don't do, too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Write feature table to disk\n",
    "dataframe_popular.to_csv('feature_tables/popular_basics.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "\n",
    "Helpful links:\n",
    "* <http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation>\n",
    "* <https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_merge_bow(dataframe):\n",
    "    \"\"\"Take a dataframe, extract '_text' and make a Bag of Words.\n",
    "    Write BoW features to their own file, then merge with input\n",
    "    and return new dataframe.\n",
    "    \n",
    "    TODO: Revisit options for CountVectorizer() (lowercase, tokenizer, min freq)\n",
    "    \"\"\"\n",
    "    # Get list of strings, for input into vectorizer\n",
    "    text_list = dataframe['_text'].tolist()\n",
    "\n",
    "    # Setup Vectorizer\n",
    "    vectorizer = CountVectorizer(min_df=1)  \n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)  # input is a list of strings, 1 per document\n",
    "\n",
    "    # Put BoW vectors into a new df\n",
    "    dataframe_bow = pandas.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # Write BoW to disk\n",
    "    # Just the Bag of Words, in case we want to use it by itself later\n",
    "    dataframe_bow.to_csv('feature_tables/popular_bow.csv', sep='|', encoding='utf-8')\n",
    "    \n",
    "    # Merge BoW df with the original feature table df\n",
    "    # Important: Make sure the concat() function uses the original id index of the first, text datafram\n",
    "    dataframe = pandas.concat([dataframe, dataframe_bow], axis=1, join_axes=[dataframe.index])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4679, 9212)\n"
     ]
    }
   ],
   "source": [
    "dataframe_popular = make_merge_bow(dataframe_popular)\n",
    "\n",
    "# See the many new columns!\n",
    "print(dataframe_popular.shape)  # (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Think about how to put into feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write entire DataFrame to csv\n",
    "\n",
    "The next notebook will pick up from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_popular.to_csv('feature_tables/popular_all.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Do everything again for the unpopular tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before del and gc: 444862464\n",
      "After del and gc : 99692544\n",
      "Difference: 3451699.2\n"
     ]
    }
   ],
   "source": [
    "# Releaese the old df for gc\n",
    "import gc\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "proc = psutil.Process(os.getpid())\n",
    "gc.collect()\n",
    "mem0 = proc.memory_info().rss\n",
    "\n",
    "del dataframe_popular\n",
    "\n",
    "gc.collect()\n",
    "mem1 = proc.memory_info().rss\n",
    "\n",
    "print('Before del and gc:', mem0/1000000, '(MB)')\n",
    "print('After del and gc: ', mem1/1000000, '(MB)')\n",
    "print('Difference:       ', (mem0 - mem1)/1000000, '(MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_all_features_for_unpopular_tweets():\n",
    "    \"\"\"Do all the steps above to make the various feature tables.\n",
    "    \n",
    "    TODO: Figure out how to prevent MemoryError at .concat(). Probably give min count to CountVectorizer()\n",
    "    \"\"\"\n",
    "    dataframe_not_popular = csv_to_df('tweets/tweets_not_popular.csv')\n",
    "    dataframe_not_popular.to_csv('feature_tables/not_popular_basics.csv', sep='|', encoding='utf-8')\n",
    "    dataframe_not_popular = add_features_to_df(dataframe_not_popular)\n",
    "    dataframe_not_popular = make_merge_bow(dataframe_not_popular)\n",
    "\n",
    "    print('Total (rows, columns):', dataframe_not_popular.shape)  # (rows, columns)\n",
    "\n",
    "    dataframe_not_popular.to_csv('feature_tables/not_popular_all.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-a6a0a64d4eb7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmake_all_features_for_unpopular_tweets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-27-ce6e98041389>\u001b[0m in \u001b[0;36mmake_all_features_for_unpopular_tweets\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mdataframe_not_popular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'feature_tables/not_popular_basics.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'|'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mdataframe_not_popular\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0madd_features_to_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe_not_popular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mdataframe_not_popular\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_merge_bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataframe_not_popular\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Total (rows, columns):'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataframe_not_popular\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# (rows, columns)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-23-08883f9f3033>\u001b[0m in \u001b[0;36mmake_merge_bow\u001b[1;34m(dataframe)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m     \u001b[1;31m# Put BoW vectors into a new df\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mdataframe_bow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mterm_document_matrix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_feature_names\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m     \u001b[1;31m# Write BoW to disk\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\compressed.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    947\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtocoo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m     \u001b[1;31m##############################################################\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\coo.pyc\u001b[0m in \u001b[0;36mtoarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;34m\"\"\"See the docstring for `spmatrix.toarray`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 274\u001b[1;33m         \u001b[0mB\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_toarray_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    275\u001b[0m         \u001b[0mfortran\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf_contiguous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    276\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfortran\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflags\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_contiguous\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\johnsky\\AppData\\Local\\Continuum\\Anaconda2\\lib\\site-packages\\scipy\\sparse\\base.pyc\u001b[0m in \u001b[0;36m_process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m    798\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    799\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 800\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    801\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    802\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__numpy_ufunc__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "make_all_features_for_unpopular_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
