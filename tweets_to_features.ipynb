{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import csv\n",
    "import datetime as dt\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_df(csv_file):\n",
    "    \"\"\"Open csv, return Pandas DataFrame.\"\"\"\n",
    "    dataframe = pandas.read_csv(csv_file, \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False,\n",
    "                            )\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_lowercase(input_str):\n",
    "    \"\"\"Lowercase input string, return.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_whitespaces(input_str):\n",
    "    \"\"\"Use re library to replace all \n",
    "    whitespaces (newlines, etc.) with a simple ' ' space.\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_puncutation(input_str):\n",
    "    \"\"\"Remove certain punctuation.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = ['himself', 'very', 'those', 'most', 'this', 'it', 'did', 'be', 'each', 'you', 'was', 'should', 'down', 'if', 'that', 'no', 'itself', 'does', 'under', 'a', 'over', 'about', 'both', 'their', 'who', 'her', 'now', 'which', 'as', 'other', 'too', 'yourselves', 'and', 'why', 'how', 'your', 'into', 'i', 'before', 'by', 'again', 'having', 'during', 'of', 'after', 'against', 'is', 'here', 't', 'above', 'so', 'doing', 'me', 'between', 'are', 'whom', 'ours', 'ourselves', 'he', 'him', 'where', 'because', 'up', 'yours', 'out', 'more', 's', 'nor', 'just', 'then', 'don', 'myself', 'my', 'while', 'these', 'some', 'yourself', 'such', 'on', 'few', 'them', 'until', 'from', 'when', 'our', 'have', 'or', 'theirs', 'off', 'through', 'the', 'same', 'any', 'its', 'not', 'below', 'has', 'had', 'am', 'been', 'will', 'at', 'being', 'there', 'than', 'to', 'she', 'but', 'what', 'for', 'can', 'own', 'an', 'they', 'his', 'with', 'we', 'only', 'in', 'were', 'hers', 'once', 'all', 'further', 'do', 'themselves', 'herself']\n",
    "\n",
    "def remove_stopwords(input_tokens):\n",
    "    \"\"\"Remove common words.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "\n",
    "Show plain function, maybe NLTK too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer\n",
    "\n",
    "def tokenize_words(input_string):\n",
    "    \"\"\"Take a string, return a list of \n",
    "    strings broken on whitespace, but do \n",
    "    not break @mentions and URLs.\n",
    "    \n",
    "    Alternative: Try using something like `[word for word in re.sub('\\W', ' ', s).split()]`.\n",
    "    then stripping punct that isn't @ or #.\n",
    "    \"\"\"\n",
    "    punctuation = [',', '!', '\"', '. ', ': ']\n",
    "    for char in punctuation:\n",
    "        input_string = input_string.replace(char, ' ')\n",
    "    \n",
    "    return [w for w in input_string.split(' ') if w]  # rm empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@CuteEmergency', \"I'm\", 'okay', 'https://t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See @users and http: not split\n",
    "a_tweet = \"\"\"@CuteEmergency: \"I'm okay!\" https://t.co/TWMwjG03Fd\"\"\"\n",
    "tokenize_words(a_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting text\n",
    "\n",
    "* count chars\n",
    "* count words\n",
    "* links\n",
    "* count links\n",
    "* #hashtags\n",
    "* count #hashtags\n",
    "* @mentions\n",
    "* count @mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `http(s)://`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('http')]`\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('http'):\n",
    "            urls.append(word)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `#`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('#')]`\n",
    "    \"\"\"\n",
    "    hashtags = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('#'):\n",
    "            hashtags.append(word)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `@`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('@')]`\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('@'):\n",
    "            mentions.append(word)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def add_features_to_df(dataframe):\n",
    "    \"\"\"Take DataFrame of tweets, extract some specific \n",
    "    features and add to returned DataFrame.\n",
    "    \"\"\"\n",
    "    #tokens = []  # list of strings\n",
    "    char_count = []\n",
    "    word_count = []\n",
    "    urls = []\n",
    "    url_counts = []\n",
    "    hashtags = []\n",
    "    hashtag_counts = []\n",
    "    mentions = []\n",
    "    mentions_counts = []\n",
    "\n",
    "    for i, row in dataframe.iterrows():\n",
    "        \n",
    "        # Text and tokens\n",
    "        tokens = tokenize_words(row['_text'])\n",
    "        char_count.append(len(row['_text']))\n",
    "        word_count.append(len(tokens))\n",
    "\n",
    "        # URLs\n",
    "        url_list = get_urls(tokens)\n",
    "        urls.append(url_list)\n",
    "        url_count = len(url_list)\n",
    "        url_counts.append(url_count)\n",
    "\n",
    "        # Hashtags\n",
    "        hashtag_list = get_hashtags(tokens)\n",
    "        hashtags.append(hashtag_list)\n",
    "        hashtag_count = len(hashtag_list)\n",
    "        hashtag_counts.append(hashtag_count)\n",
    "\n",
    "        # Mentions\n",
    "        mentions_list = get_mentions(tokens)\n",
    "        mentions.append(mentions_list)\n",
    "        mentions_count = len(mentions_list)\n",
    "        mentions_counts.append(mentions_count)\n",
    "\n",
    "\n",
    "    dataframe['_char_count'] = char_count\n",
    "    dataframe['_word_count'] = word_count\n",
    "    dataframe['_urls'] = urls\n",
    "    dataframe['_url_count'] = url_counts\n",
    "    dataframe['_hashtags'] = hashtags\n",
    "    dataframe['_hashtag_count'] = hashtag_counts\n",
    "    dataframe['_mentions'] = mentions\n",
    "    dataframe['_mentions_count'] = mentions_counts\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Named entity recognition (NER)\n",
    "\n",
    "Maybe show NLTK code, but don't do, too slow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "\n",
    "Helpful links:\n",
    "* <http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation>\n",
    "* <https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words>\n",
    "\n",
    "Brief example: <https://github.com/kylepjohnson/lecture_nyc_ascent/blob/master/code_snippets/Example%20-%20Bag%20of%20words%20and%20Pandas%20df%20concat().ipynb>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_merge_bow_write(dataframe, save_path):\n",
    "    \"\"\"Take a dataframe, extract '_text' and make a Bag of Words.\n",
    "    Write BoW features to their own file, then merge with input\n",
    "    and return new dataframe.\n",
    "    \n",
    "    TODO: Revisit options for CountVectorizer() (lowercase, tokenizer, min freq)\n",
    "    \"\"\"\n",
    "    # Get list of strings, for input into vectorizer\n",
    "    text_list = dataframe['_text'].tolist()\n",
    "\n",
    "    # Setup Vectorizer\n",
    "    # Note that min_df is confusing; see http://stackoverflow.com/a/27697863\n",
    "    # min_df + an integer: if word found in less than n docs, then ignore\n",
    "    vectorizer = CountVectorizer(min_df=2)  \n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)  # input is a list of strings, 1 per document\n",
    "\n",
    "    # Put BoW vectors into a new df\n",
    "    dataframe_bow = pandas.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # Write BoW to disk\n",
    "    # Just the Bag of Words, in case we want to use it by itself later\n",
    "    # TODO! Add '_popular' column to this, or ditch this csv altogether\n",
    "    dataframe_bow.to_csv(save_path, sep='|', encoding='utf-8')\n",
    "    \n",
    "    # Merge BoW df with the original feature table df\n",
    "    # Important: Make sure the concat() function uses the original id index of the first, text datafram\n",
    "    dataframe = pandas.concat([dataframe, dataframe_bow], axis=1, join_axes=[dataframe.index])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_merge_bow(dataframe):\n",
    "    \"\"\"Take a dataframe, extract '_text' and make a Bag of Words.\n",
    "    Write BoW features to their own file, then merge with input\n",
    "    and return new dataframe.\n",
    "    \n",
    "    TODO: Revisit options for CountVectorizer() (lowercase, tokenizer, min freq)\n",
    "    \"\"\"\n",
    "    # Get list of strings, for input into vectorizer\n",
    "    text_list = dataframe['_text'].tolist()\n",
    "\n",
    "    # Setup Vectorizer\n",
    "    # Note that min_df is confusing; see http://stackoverflow.com/a/27697863\n",
    "    # min_df + an integer: if word found in less than n docs, then ignore\n",
    "    vectorizer = CountVectorizer(min_df=2)  \n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)  # input is a list of strings, 1 per document\n",
    "\n",
    "    # Put BoW vectors into a new df\n",
    "    dataframe_bow = pandas.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    return dataframe_bow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Think about how to put into feature table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "See <https://github.com/kylepjohnson/lecture_nyc_ascent/blob/master/code_snippets/Example%20-%20Topic%20modeling.ipynb> for example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write entire DataFrame to csv\n",
    "\n",
    "The next notebook will pick up from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Do everything again for the unpopular tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_all_features_for_tweets():\n",
    "    \"\"\"Do all the steps to create one feature \n",
    "    table of popular and unpopular tweets.\n",
    "    \"\"\"\n",
    "    \n",
    "    print('Startting feature extraction ...')\n",
    "    t0 = dt.datetime.utcnow()\n",
    "    # Make sure 'feature_tables' present\n",
    "    features_dir = 'feature_tables'\n",
    "    if not os.path.isdir(features_dir):\n",
    "        os.mkdir(features_dir)\n",
    "\n",
    "    # load csvs to dfs\n",
    "    dataframe_popular = csv_to_df('tweets/tweets_popular.csv')\n",
    "    dataframe_not_popular = csv_to_df('tweets/tweets_not_popular.csv')\n",
    "    \n",
    "    # Remove dupes\n",
    "    dataframe_popular = dataframe_popular.drop_duplicates()\n",
    "    dataframe_not_popular = dataframe_not_popular.drop_duplicates()\n",
    "    \n",
    "    # Add column '_popular' or '_unpopular' for each df\n",
    "    dataframe_popular['_popular'] = True\n",
    "    dataframe_not_popular['_popular'] = False\n",
    "    \n",
    "    # Append unpopular to popular df\n",
    "    dataframe = pandas.concat([dataframe_popular, dataframe_not_popular])\n",
    "    \n",
    "    \n",
    "    # Extract features from df, add back to df\n",
    "    dataframe = add_features_to_df(dataframe)\n",
    "    \n",
    "    # Write df, now with basic extracted features, to .csv\n",
    "    dataframe.to_csv('feature_tables/basics.csv', sep='|', encoding='utf-8')\n",
    "\n",
    "    # Make BoW df, then write it to .csv\n",
    "    dataframe_bow = make_merge_bow(dataframe)\n",
    "    \n",
    "    # Just the Bag of Words, in case we want to use it by itself later\n",
    "    dataframe_bow.to_csv('feature_tables/bow.csv', sep='|', encoding='utf-8')\n",
    "    \n",
    "    # Merge BoW df with the original feature table df\n",
    "    # Important: Make sure the concat() function uses the original id index of the first, text df\n",
    "    dataframe = pandas.concat([dataframe, dataframe_bow], axis=1, join_axes=[dataframe.index])\n",
    "    dataframe.to_csv('feature_tables/all.csv', sep='|', encoding='utf-8')\n",
    "    \n",
    "    print('... completed in {}.'.format(dt.datetime.utcnow() - t0))\n",
    "    print('Total (rows, columns):', dataframe.shape)  # (rows, columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (rows, columns): (22706, 11014)\n"
     ]
    }
   ],
   "source": [
    "make_all_features_for_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
