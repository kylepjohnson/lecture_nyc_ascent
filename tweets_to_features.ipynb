{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import csv\n",
    "from datetime import datetime\n",
    "import os\n",
    "import re\n",
    "\n",
    "import pandas\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Open with Pandas, load into DataFrame\n",
    "\n",
    "# TODO: Parse dates correctly; this is close but not working\n",
    "date_parser = lambda x: pandas.datetime.strptime(x, '%a %b %d %H:%M:%S +z %Y')  # Mon Feb 15 20:44:33 +0000 2016\n",
    "\n",
    "popular_df = pandas.read_csv('tweets/tweets_popular.csv', \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False, \n",
    "                             parse_dates=True,\n",
    "                             date_parser=date_parser\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4679, 3)\n",
      "Columns: Index([u'_text', u'_rt_count', u'_tweet_datetime'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Let's inspect our data\n",
    "print('Shape:', popular_df.shape)\n",
    "print('Columns:', popular_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text column\n",
      "0    @CringeLMAO: Easy there m8 https://t.co/dnF3Wq...\n",
      "1    @AustinMahone: Just posted a photo https://t.c...\n",
      "2    @Ashton5SOS: Some days I drink way to much cof...\n",
      "3    @lailamuhammad: When you nail that #Beyonc   m...\n",
      "Name: _text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Text column')\n",
    "print(popular_df['_text'][:4])  # the underscores will be apparent later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retweet count\n",
      "0     2084\n",
      "1     1059\n",
      "2    24121\n",
      "3      801\n",
      "Name: _rt_count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Retweet count')\n",
    "print(popular_df['_rt_count'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date-time\n",
      "0    Mon Feb 15 20:44:33 +0000 2016\n",
      "1    Mon Feb 15 20:44:33 +0000 2016\n",
      "2    Mon Feb 15 20:44:33 +0000 2016\n",
      "3    Mon Feb 15 20:44:33 +0000 2016\n",
      "Name: _tweet_datetime, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Look at columns\n",
    "print('Date-time')\n",
    "print(popular_df['_tweet_datetime'][:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'str'>\n"
     ]
    }
   ],
   "source": [
    "# Let's look at the parsed date-time\n",
    "# TODO: Try to parse this right\n",
    "dt = popular_df['_tweet_datetime'][0]\n",
    "print(type(dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's do this again, but with a function\n",
    "del popular_df  # rm large object from memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def csv_to_df(csv_file):\n",
    "    \"\"\"Open csv, return Pandas DataFrame.\"\"\"\n",
    "    dataframe = pandas.read_csv(csv_file, \n",
    "                             delimiter='|', \n",
    "                             error_bad_lines=False, \n",
    "                             warn_bad_lines=False, \n",
    "                             parse_dates=True,\n",
    "                             date_parser=date_parser\n",
    "                            )\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_popular = csv_to_df('tweets/tweets_popular.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Data cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove duplicate rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before: (4679, 3)\n",
      "Shape after: (4679, 3)\n"
     ]
    }
   ],
   "source": [
    "# rows, columns\n",
    "print('Shape before:', dataframe_popular.shape)\n",
    "\n",
    "popular_df = dataframe_popular.drop_duplicates()\n",
    "print('Shape after:', dataframe_popular.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other cleanup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Todo\n",
    "\n",
    "# case, space, some punctuation, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word tokenization\n",
    "\n",
    "Show plain function, maybe NLTK too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# A basic tokenizer\n",
    "\n",
    "def tokenize_words(input_string):\n",
    "    \"\"\"Take a string, return a list of \n",
    "    strings broken on whitespace, but do \n",
    "    not break @mentions and URLs.\n",
    "    \"\"\"\n",
    "    punctuation = [',', '!', '\"', '. ', ': ']\n",
    "    for char in punctuation:\n",
    "        input_string = input_string.replace(char, ' ')\n",
    "    \n",
    "    return [w for w in input_string.split(' ') if w]  # rm empty strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@CuteEmergency', \"I'm\", 'okay', 'https://t.co/TWMwjG03Fd']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See @users and http: not split\n",
    "a_tweet = \"\"\"@CuteEmergency: \"I'm okay!\" https://t.co/TWMwjG03Fd\"\"\"\n",
    "tokenize_words(a_tweet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting text\n",
    "\n",
    "* count chars\n",
    "* count words\n",
    "* links\n",
    "* count links\n",
    "* #hashtags\n",
    "* count #hashtags\n",
    "* @mentions\n",
    "* count @mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_urls(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `http(s)://`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('http')]`\n",
    "    \"\"\"\n",
    "    urls = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('http'):\n",
    "            urls.append(word)\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_hashtags(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `#`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('#')]`\n",
    "    \"\"\"\n",
    "    hashtags = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('#'):\n",
    "            hashtags.append(word)\n",
    "    return hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_mentions(input_tokens):\n",
    "    \"\"\"Check incoming list of strings, check if token\n",
    "    starts with `@`.\n",
    "    \n",
    "    Could be done with list comprehension, too:\n",
    "    `[w for w in input_tokens if word.startswith('@')]`\n",
    "    \"\"\"\n",
    "    mentions = []\n",
    "    for word in input_tokens:\n",
    "        if word.startswith('@'):\n",
    "            mentions.append(word)\n",
    "    return mentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_features_to_df(dataframe):\n",
    "    \"\"\"Take DataFrame of tweets, extract some specific \n",
    "    features and add to returned DataFrame.\n",
    "    \"\"\"\n",
    "    #tokens = []  # list of strings\n",
    "    char_count = []\n",
    "    word_count = []\n",
    "    urls = []\n",
    "    hashtags = []\n",
    "    mentions = []\n",
    "\n",
    "    for i, row in dataframe.iterrows():\n",
    "        \n",
    "        # Text and tokens\n",
    "        tokens = tokenize_words(row['_text'])\n",
    "        char_count.append(len(row['_text']))\n",
    "        word_count.append(len(tokens))\n",
    "\n",
    "        # URLs\n",
    "        url_list = get_urls(tokens)\n",
    "        urls.append(url_list)\n",
    "        url_count = len(url_list)\n",
    "\n",
    "        # Hashtags\n",
    "        hashtag_list = get_hashtags(tokens)\n",
    "        hashtags.append(hashtag_list)\n",
    "        hashtag_count = len(hashtag_list)\n",
    "\n",
    "        # Mentions\n",
    "        mentions_list = get_mentions(tokens)\n",
    "        mentions.append(mentions_list)\n",
    "        mentions_count = len(mentions_list)\n",
    "\n",
    "    #dataframe['_tokens'] = tokens\n",
    "    dataframe['_char_count'] = char_count\n",
    "    dataframe['_word_count'] = word_count\n",
    "    dataframe['_urls'] = urls\n",
    "    dataframe['_url_count'] = url_count\n",
    "    dataframe['_hashtags'] = hashtags\n",
    "    dataframe['_hashtag_count'] = hashtag_count\n",
    "    dataframe['_mentions'] = hashtags\n",
    "    dataframe['_mentions_count'] = mentions_count\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataframe_popular = add_features_to_df(dataframe_popular)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (4679, 11)\n",
      "Columns: Index([u'_text', u'_rt_count', u'_tweet_datetime', u'_char_count',\n",
      "       u'_word_count', u'_urls', u'_url_count', u'_hashtags',\n",
      "       u'_hashtag_count', u'_mentions', u'_mentions_count'],\n",
      "      dtype='object')\n",
      "Index([u'_text', u'_rt_count', u'_tweet_datetime', u'_char_count',\n",
      "       u'_word_count', u'_urls', u'_url_count', u'_hashtags',\n",
      "       u'_hashtag_count', u'_mentions', u'_mentions_count'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print('Shape:', dataframe_popular.shape)\n",
    "print('Columns:', dataframe_popular.columns)\n",
    "print(dataframe_popular.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Extract from datetime\n",
    "\n",
    "Our times are not very diverse, so will not useful for the feature table in this data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO: Named entity recognition (NER)\n",
    "\n",
    "Maybe show NLTK code, but don't do, too slow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make sure 'feature_tables' present\n",
    "features_dir = 'feature_tables'\n",
    "if not os.path.isdir(features_dir):\n",
    "    os.mkdir(features_dir)\n",
    "\n",
    "# Write feature table to disk\n",
    "dataframe_popular.to_csv('feature_tables/popular_basics.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words\n",
    "\n",
    "Helpful links:\n",
    "* <http://scikit-learn.org/stable/modules/feature_extraction.html#the-bag-of-words-representation>\n",
    "* <https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-1-for-beginners-bag-of-words>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def make_merge_bow(dataframe, save_path):\n",
    "    \"\"\"Take a dataframe, extract '_text' and make a Bag of Words.\n",
    "    Write BoW features to their own file, then merge with input\n",
    "    and return new dataframe.\n",
    "    \n",
    "    TODO: Revisit options for CountVectorizer() (lowercase, tokenizer, min freq)\n",
    "    \"\"\"\n",
    "    # Get list of strings, for input into vectorizer\n",
    "    text_list = dataframe['_text'].tolist()\n",
    "\n",
    "    # Setup Vectorizer\n",
    "    # Note that min_df is confusing; see http://stackoverflow.com/a/27697863\n",
    "    # min_df + an integer: if word found in less than n docs, then ignore\n",
    "    vectorizer = CountVectorizer(min_df=2)  \n",
    "    term_document_matrix = vectorizer.fit_transform(text_list)  # input is a list of strings, 1 per document\n",
    "\n",
    "    # Put BoW vectors into a new df\n",
    "    dataframe_bow = pandas.DataFrame(term_document_matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "    \n",
    "    # Write BoW to disk\n",
    "    # Just the Bag of Words, in case we want to use it by itself later\n",
    "    dataframe_bow.to_csv(save_path, sep='|', encoding='utf-8')\n",
    "    \n",
    "    # Merge BoW df with the original feature table df\n",
    "    # Important: Make sure the concat() function uses the original id index of the first, text datafram\n",
    "    dataframe = pandas.concat([dataframe, dataframe_bow], axis=1, join_axes=[dataframe.index])\n",
    "    \n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4679, 3871)\n"
     ]
    }
   ],
   "source": [
    "dataframe_popular = make_merge_bow(dataframe_popular, 'feature_tables/popular_bow.csv')\n",
    "\n",
    "# See the many new columns!\n",
    "print(dataframe_popular.shape)  # (rows, columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic modeling\n",
    "\n",
    "Think about how to put into feature table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write entire DataFrame to csv\n",
    "\n",
    "The next notebook will pick up from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataframe_popular.to_csv('feature_tables/popular_all.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Do everything again for the unpopular tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before del and gc: 244.195328 (MB)\n",
      "After del and gc:  99.418112 (MB)\n",
      "Difference:        144.777216 (MB)\n"
     ]
    }
   ],
   "source": [
    "# Ignore this code unless you run into MemoryError while running the unpopular tweets\n",
    "# Releaese the old df for gc\n",
    "import gc\n",
    "import psutil\n",
    "\n",
    "proc = psutil.Process(os.getpid())\n",
    "gc.collect()\n",
    "mem0 = proc.memory_info().rss\n",
    "\n",
    "del dataframe_popular\n",
    "\n",
    "gc.collect()\n",
    "mem1 = proc.memory_info().rss\n",
    "\n",
    "print('Before del and gc:', mem0/1000000, '(MB)')\n",
    "print('After del and gc: ', mem1/1000000, '(MB)')\n",
    "print('Difference:       ', (mem0 - mem1)/1000000, '(MB)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_all_features_for_unpopular_tweets():\n",
    "    \"\"\"Do all the steps above to make the various feature tables.\n",
    "    \"\"\"\n",
    "    dataframe_not_popular = csv_to_df('tweets/tweets_not_popular.csv')\n",
    "    dataframe_not_popular.to_csv('feature_tables/not_popular_basics.csv', sep='|', encoding='utf-8')\n",
    "    dataframe_not_popular = add_features_to_df(dataframe_not_popular)\n",
    "    dataframe_not_popular = make_merge_bow(dataframe_not_popular, 'feature_tables/not_popular_bow.csv')\n",
    "\n",
    "    print('Total (rows, columns):', dataframe_not_popular.shape)  # (rows, columns)\n",
    "\n",
    "    dataframe_not_popular.to_csv('feature_tables/not_popular_all.csv', sep='|', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total (rows, columns): (18618, 9303)\n"
     ]
    }
   ],
   "source": [
    "make_all_features_for_unpopular_tweets()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
